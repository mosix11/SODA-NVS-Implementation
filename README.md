# Minimal Implementation of SODA for Novel View Synthesis

This repository contains a minimal implementation of the SODA method proposed in the paper [SODA: Bottleneck Diffusion Models for Representation Learning](https://arxiv.org/abs/2311.17901). This implementation is just a simplified version of the Novel View Synthesis task on 3D datasets experimented in the paper. I used NMR (Neural 3D Mesh Renderer) dataset hosted by [Niemeyer et al](https://github.com/autonomousvision/differentiable_volumetric_rendering) using SoftRas split. It could be applicable to other datasets aswell but the configuration files need to be modified.

## Table of Contents

- [Minimal Implementation of SODA for Novel View Synthesis](#minimal-implementation-of-soda-for-novel-view-synthesis)
  - [Table of Contents](#table-of-contents)
  - [Introduction](#introduction)
  - [Simplifications](#simplifications)
  - [Issues](#issues)
  - [Installation](#installation)
  - [Usage](#usage)
    - [Training](#training)
    - [Testing](#testing)
  - [References](#references)

## Introduction

The SODA model consists of using diffusion models for representation learning. More specifically, in this approach, we train an encoder jointly with the diffusion denoiser and the task of encoder is to produce latent vectors that guide the denoiser twoards generating intended targets. Later we can use the encoder (which has learnt a compressed representation of the data) for downstream tasks.

:exclamation: This implementation only focuses on NVS task while the paper includes a variety of tasks which show the power of SODA.

## Simplifications

Here is the differences with the original paper:

1. I have not implemented the Cross-Attention mechanism for conditioning on pose information (2D ray-grids in case of NVS) as mentioned in the original paper (it improves the performance on 3D datasets).
2. Despite the original paper suggesting to split the latent vector to m+1 subvectors and do layer modulation for each layer in the denoiser U-Net with its corresponding latent subvector (this helpes latent disentanglement which was not the goal of my project), I used the whole latent vector for feature modulation using AdaGN.
3. The original paper upsamples the images to 256x256 for the encoder and 128x128 for the denoiser (NMR images are 64x64) however I trained both networks with 64x64 images.
4. In the original paper the authors state that after calculating the HxWx6 dimensional ray-grids for each view of an object, they project each 6 dimensional ray = \[o,d\] to a sphere which I did not exactly understand what do they mean by that! And it seems the end encoding should be 512 dimensional vector, however, I used the 6 dimensional rays and after using scaled sinusoidal positional encoding each ray is converted to 288 dimensionl vectors (can be adjusted with frequency band L) using NeRF style encoding .
5. The AdaGN trick for modulation using the laten vector is done using linear projections of the latent vector and time embedding in the original paper while in this project it is done simply by chunking the vectors (latent and time embedding) into two vectors of the same shape.

## Issues

Currently, the modle fails to generat high quality samples, but the generated camera views are rather accurate. Unfortunately, due to high computaational cost of training, I could not experiment with different hyperparameters and tune the model to generate samples like those demonstrated in the paper. I assume the encoder is doing its job, but the denoiser is not powerful enough to generate high quality samples.

## Installation

In addition to the requirements in requirements.txt file the project needs `p7zip` to extract the dataset files faster!

1. Clone the repository:

    ```bash
    git clone https://github.com/mosix11/SODA-NVS-Implementation.git
    ```

2. Install dependencies:

    ```bash
    pip install -r requirements.txt
    ```

## Usage

### Training

The training configuration is specified in the `configs/NMR.yaml` file. You can change it if you want to modify the model or training hyperparameters.
To start the training, you simply run the train script:

```bash
python train.py
```

### Testing

To sample all 24 views of an object from the test set, you can run the sample script by inputting the an object index or the script will take a random sample form the test set and generates 24 target views for this sample using one source view.

```bash
python sample.py --objidx 3
```

To evaluate the classification performance of the trained encoder, you can run the evaluation script:

```bash
python evaluate.py --type lp
```

To evaluate the FID score of the images generated by the diffusion model guided by the encoder, you can run the evaluation script:

```bash
python evaluate.py --type FID
```

## References

```bibtex
@article{hudson2023soda,
  title={SODA: Bottleneck Diffusion Models for Representation Learning},
  author={Hudson, Drew A and Zoran, Daniel and Malinowski, Mateusz and Lampinen, Andrew K and Jaegle, Andrew and McClelland, James L and Matthey, Loic and Hill, Felix and Lerchner, Alexander},
  journal={arXiv preprint arXiv:2311.17901},
  year={2023}
}
```
